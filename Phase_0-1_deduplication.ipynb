{"cells":[{"cell_type":"markdown","id":"6d65c47c","metadata":{"id":"6d65c47c"},"source":["# Phase 0-1: Dataset Deduplication\n","\n","Utilities for image deduplication."]},{"cell_type":"code","execution_count":null,"id":"bad1916a","metadata":{"id":"bad1916a"},"outputs":[],"source":["# Core imports\n","import os\n","import hashlib\n","from pathlib import Path\n","from collections import defaultdict\n","from datetime import datetime\n","import shutil\n","\n","from tqdm import tqdm  # progress bars\n","\n","print(f\"Imports OK. CWD: {Path().resolve()}\")"]},{"cell_type":"code","execution_count":null,"id":"6ce19873","metadata":{"id":"6ce19873"},"outputs":[],"source":["# Paths are machine-specific. Adjust PROJECT_ROOT before running on a new machine.\n","PROJECT_ROOT = \"/Users/tyreecruse/Desktop/CS230/Project/Data/Original\"\n","# Alternative:\n","# PROJECT_ROOT = os.getcwd()\n","\n","CONFIG = {\n","    # Dataset directory (input and output - modified in-place)\n","    \"dataset_path\": os.path.join(PROJECT_ROOT, \"master_dataset_pool\"),\n","\n","    # Backup settings\n","    \"create_backup\": True,\n","    \"backup_dir\": os.path.join(PROJECT_ROOT, \"dataset_backups\"),\n","\n","    # Processing parameters\n","    \"chunk_size\": 64 * 1024,  # bytes\n","    \"remove_labels\": True,    # remove labels for duplicate images\n","}\n","\n","print(\"Config\")\n","print(\"------\")\n","for key in (\"dataset_path\", \"create_backup\", \"backup_dir\", \"chunk_size\"):\n","    print(f\"{key:>14}: {CONFIG[key]}\")\n","\n","# Quick sanity check on the dataset\n","dataset_path = Path(CONFIG[\"dataset_path\"])\n","images_dir = dataset_path / \"images\"\n","\n","if not images_dir.exists():\n","    print(f\"\\n[warning] images/ not found under {dataset_path}\")\n","    print(\"          Run consolidation or adjust CONFIG['dataset_path'].\")\n","else:\n","    n_images = sum(1 for p in images_dir.iterdir() if p.is_file())\n","    print(f\"\\nFound {n_images:,} image files under {images_dir}\")"]},{"cell_type":"code","execution_count":null,"id":"c18465ae","metadata":{"id":"c18465ae"},"outputs":[],"source":["def get_file_hash(path, chunk_size=65536):\n","    \"\"\"Return MD5 hex digest of the file at `path`.\"\"\"\n","    h = hashlib.md5()\n","    with open(path, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()"]},{"cell_type":"code","execution_count":null,"id":"4cbaa0a6","metadata":{"id":"4cbaa0a6"},"outputs":[],"source":["def create_backup(dataset_path, backup_dir):\n","    \"\"\"Copy dataset_path into backup_dir/<name>_backup_<timestamp> and return the new path.\"\"\"\n","    dataset_path = Path(dataset_path)\n","    backup_dir = Path(backup_dir)\n","    backup_dir.mkdir(parents=True, exist_ok=True)\n","\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    backup_path = backup_dir / f\"{dataset_path.name}_backup_{ts}\"\n","\n","    print(f\"\\nCreating backup at {backup_path}\")\n","    shutil.copytree(dataset_path, backup_path)\n","\n","    return backup_path"]},{"cell_type":"code","execution_count":null,"id":"8aa4a2f6","metadata":{"id":"8aa4a2f6"},"outputs":[],"source":["def deduplicate_dataset(dataset_dir, config):\n","    \"\"\"Deduplicate images (and optionally labels) in-place; return a stats dict.\"\"\"\n","    dataset_dir = Path(dataset_dir)\n","    images_dir = dataset_dir / \"images\"\n","    labels_dir = dataset_dir / \"labels\"\n","\n","    if not images_dir.is_dir():\n","        raise FileNotFoundError(f\"images/ directory not found: {images_dir}\")\n","\n","    image_files = [p for p in images_dir.iterdir() if p.is_file()]\n","\n","    stats = {\n","        \"total_images\": len(image_files),\n","        \"duplicates_removed\": 0,\n","        \"labels_removed\": 0,\n","        \"unique_images\": 0,\n","        \"duplicate_groups\": 0,\n","    }\n","\n","    if not image_files:\n","        print(f\"No files found under {images_dir}\")\n","        return stats\n","\n","    print(f\"\\nScanning {images_dir} for duplicates \"\n","          f\"({stats['total_images']:,} files)\")\n","\n","    # Build hash map -> list of files with that hash\n","    hash_to_files = defaultdict(list)\n","    for img_path in tqdm(image_files, desc=\"hash\", unit=\"file\"):\n","        try:\n","            h = get_file_hash(img_path, config[\"chunk_size\"])\n","        except Exception as exc:\n","            print(f\"[hash] skip {img_path.name}: {exc}\")\n","            continue\n","        hash_to_files[h].append(img_path)\n","\n","    # Only keep entries that actually have duplicates\n","    duplicate_groups = [paths for paths in hash_to_files.values()\n","                        if len(paths) > 1]\n","    stats[\"duplicate_groups\"] = len(duplicate_groups)\n","\n","    if not duplicate_groups:\n","        print(\"No duplicate hashes found.\")\n","        stats[\"unique_images\"] = stats[\"total_images\"]\n","        return stats\n","\n","    remove_labels = config.get(\"remove_labels\", True) and labels_dir.is_dir()\n","\n","    # Remove everything except the first file in each duplicate group\n","    for paths in tqdm(duplicate_groups, desc=\"remove\", unit=\"group\"):\n","        for dup in paths[1:]:\n","            try:\n","                dup.unlink()\n","                stats[\"duplicates_removed\"] += 1\n","            except Exception as exc:\n","                print(f\"[rm] could not remove {dup.name}: {exc}\")\n","                continue\n","\n","            if remove_labels:\n","                label_path = labels_dir / f\"{dup.stem}.txt\"\n","                if label_path.exists():\n","                    try:\n","                        label_path.unlink()\n","                        stats[\"labels_removed\"] += 1\n","                    except Exception as exc:\n","                        print(f\"[rm] could not remove label {label_path.name}: {exc}\")\n","\n","    stats[\"unique_images\"] = stats[\"total_images\"] - stats[\"duplicates_removed\"]\n","    return stats"]},{"cell_type":"code","execution_count":null,"id":"e61c8806","metadata":{"id":"e61c8806"},"outputs":[],"source":["def main():\n","    \"\"\"Run optional backup and deduplication, returning a results dict.\"\"\"\n","    results = {}\n","    dataset_dir = Path(CONFIG[\"dataset_path\"])\n","\n","    try:\n","        # Optional backup\n","        if CONFIG.get(\"create_backup\", False):\n","            backup_path = create_backup(dataset_dir, CONFIG[\"backup_dir\"])\n","            results[\"backup_path\"] = str(backup_path)\n","\n","        # Deduplication\n","        stats = deduplicate_dataset(dataset_dir, CONFIG)\n","        results[\"deduplication\"] = stats\n","\n","        # Compact summary\n","        print(\"\\nDeduplication summary\")\n","        print(\"-\" * 30)\n","        print(f\"  original images : {stats['total_images']:,}\")\n","        print(f\"  removed images  : {stats['duplicates_removed']:,}\")\n","        print(f\"  removed labels  : {stats['labels_removed']:,}\")\n","        print(f\"  unique images   : {stats['unique_images']:,}\")\n","\n","        if stats[\"total_images\"]:\n","            rate = 100 * stats[\"duplicates_removed\"] / stats[\"total_images\"]\n","            print(f\"  deduplication   : {rate:.1f}%\")\n","\n","        print(f\"\\nData location     : {dataset_dir}\")\n","        if \"backup_path\" in results:\n","            print(f\"Backup            : {results['backup_path']}\")\n","\n","    except Exception as exc:\n","        print(f\"\\nDeduplication failed: {exc}\")\n","        results[\"error\"] = str(exc)\n","\n","    return results\n","\n","\n","if __name__ == \"__main__\":\n","    deduplication_results = main()"]},{"cell_type":"code","execution_count":null,"id":"26803336","metadata":{"id":"26803336"},"outputs":[],"source":["dataset_path = Path(CONFIG[\"dataset_path\"])\n","images_dir = dataset_path / \"images\"\n","labels_dir = dataset_path / \"labels\"\n","\n","print(\"\\nDataset check\")\n","print(\"-------------\")\n","\n","if images_dir.is_dir():\n","    image_files = [p for p in images_dir.iterdir() if p.is_file()]\n","    label_files = list(labels_dir.glob(\"*.txt\")) if labels_dir.is_dir() else []\n","\n","    n_img, n_lbl = len(image_files), len(label_files)\n","    print(f\"  images : {n_img:,}\")\n","    print(f\"  labels : {n_lbl:,}\")\n","    if n_img:\n","        cov = 100 * n_lbl / n_img\n","        print(f\"  label/image: {cov:.1f}%\")\n","\n","    if image_files:\n","        print(\"\\n  sample files:\")\n","        for p in image_files[:5]:\n","            print(\"   -\", p.name)\n","        if n_img > 5:\n","            print(f\"   ... (+{n_img - 5:,} more)\")\n","else:\n","    print(f\"  images directory not found at {images_dir}\")\n","\n","# Tie back to the run stats, if available\n","if 'deduplication_results' in globals() and isinstance(deduplication_results, dict):\n","    stats = deduplication_results['deduplication']\n","    print(\"\\nRun summary\")\n","    print(\"-----------\")\n","    print(f\"  removed images : {stats['duplicates_removed']:,}\")\n","    print(f\"  removed labels : {stats['labels_removed']:,}\")\n","    print(f\"  unique images  : {stats['unique_images']:,}\")"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}