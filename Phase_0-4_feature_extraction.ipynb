{"cells":[{"cell_type":"markdown","id":"6db92b33","metadata":{"id":"6db92b33"},"source":["# Phase 0-4: Feature Extraction\n","\n","Extract CLIP features for all images in the dataset and save them to features.pkl."]},{"cell_type":"code","execution_count":null,"id":"54f54ef6","metadata":{"id":"54f54ef6"},"outputs":[],"source":["import os\n","import pickle\n","from pathlib import Path\n","from datetime import datetime\n","\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import torch\n","\n","print(f\"Imports OK. CWD: {Path().resolve()}\")\n","print(f\"GPU available: {torch.cuda.is_available()}\")"]},{"cell_type":"code","execution_count":null,"id":"b558e74e","metadata":{"id":"b558e74e"},"outputs":[],"source":["# CLIP import helper\n","try:\n","    import clip  # OpenAI CLIP: pip install git+https://github.com/openai/CLIP.git\n","except ImportError:\n","    clip = None\n","    print(\"\\n[warning] `clip` package not found. Install with:\")\n","    print(\"  pip install git+https://github.com/openai/CLIP.git\")"]},{"cell_type":"code","execution_count":null,"id":"71705ada","metadata":{"id":"71705ada"},"outputs":[],"source":["# Paths are machine-specific. Adjust PROJECT_ROOT before running on a new machine.\n","PROJECT_ROOT = \"/Users/tyreecruse/Desktop/CS230/Project/Data/Original\"\n","# Or just use the current directory:\n","# PROJECT_ROOT = os.getcwd()\n","\n","CONFIG = {\n","    \"dataset_path\": os.path.join(PROJECT_ROOT, \"master_dataset_pool\"),\n","    \"output_path\": os.path.join(PROJECT_ROOT, \"features.pkl\"),\n","\n","    \"model_name\": \"ViT-B/32\",\n","    \"batch_size\": 512,\n","    \"normalize\": True,\n","}\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(\"Config\")\n","print(\"------\")\n","print(f\"dataset_path: {CONFIG['dataset_path']}\")\n","print(f\"output_path : {CONFIG['output_path']}\")\n","print(f\"model_name  : {CONFIG['model_name']}\")\n","print(f\"batch_size  : {CONFIG['batch_size']}\")\n","print(f\"device      : {device}\")\n","\n","dataset_path = Path(CONFIG[\"dataset_path\"])\n","images_dir = dataset_path / \"images\"\n","\n","if not images_dir.exists():\n","    print(f\"\\n[warning] images/ not found under {dataset_path}\")\n","else:\n","    n_images = sum(1 for p in images_dir.iterdir() if p.is_file())\n","    print(f\"\\nFound {n_images:,} image files under {images_dir}\")"]},{"cell_type":"code","execution_count":null,"id":"6b2bd864","metadata":{"id":"6b2bd864"},"outputs":[],"source":["def list_image_files(images_dir):\n","    \"\"\"Return a sorted list of image file paths under images_dir.\"\"\"\n","    images_dir = Path(images_dir)\n","    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n","    files = [p for p in images_dir.iterdir() if p.suffix.lower() in exts and p.is_file()]\n","    return sorted(files)"]},{"cell_type":"code","execution_count":null,"id":"6d6662bb","metadata":{"id":"6d6662bb"},"outputs":[],"source":["def load_clip_model(model_name, device=\"cpu\"):\n","    \"\"\"Load CLIP model + preprocess transform.\"\"\"\n","    if clip is None:\n","        raise ImportError(\"clip package is not installed.\")\n","    model, preprocess = clip.load(model_name, device=device)\n","    model.eval()\n","    return model, preprocess"]},{"cell_type":"code","execution_count":null,"id":"f5fffe3f","metadata":{"id":"f5fffe3f"},"outputs":[],"source":["def extract_features_for_batch(model, preprocess, device, paths):\n","    \"\"\"Extract features for a list of image paths (single batch).\"\"\"\n","    images = []\n","    valid_paths = []\n","    for p in paths:\n","        try:\n","            img = Image.open(p).convert(\"RGB\")\n","        except Exception:\n","            continue\n","        images.append(preprocess(img))\n","        valid_paths.append(str(p))\n","\n","    if not images:\n","        return None, []\n","\n","    batch = torch.stack(images).to(device)\n","    with torch.no_grad():\n","        feats = model.encode_image(batch)\n","        if feats.ndim > 2:\n","            feats = feats.mean(dim=1)\n","        feats = feats.cpu().numpy()\n","\n","    return feats, valid_paths"]},{"cell_type":"code","execution_count":null,"id":"fd5556a8","metadata":{"id":"fd5556a8"},"outputs":[],"source":["def process_all_images(image_files, model, preprocess, device, batch_size, normalize=True):\n","    \"\"\"Process all images in batches and return (features, paths).\"\"\"\n","    all_features = []\n","    all_paths = []\n","\n","    n = len(image_files)\n","    for start in tqdm(range(0, n, batch_size), desc=\"batches\"):\n","        end = min(start + batch_size, n)\n","        batch_paths = image_files[start:end]\n","        feats, valid_paths = extract_features_for_batch(model, preprocess, device, batch_paths)\n","        if feats is None:\n","            continue\n","        all_features.append(feats)\n","        all_paths.extend(valid_paths)\n","\n","    if not all_features:\n","        return np.zeros((0, 0), dtype=np.float32), []\n","\n","    features = np.concatenate(all_features, axis=0).astype(np.float32)\n","\n","    if normalize and features.size > 0:\n","        norms = np.linalg.norm(features, axis=1, keepdims=True) + 1e-8\n","        features = features / norms\n","\n","    return features, all_paths"]},{"cell_type":"code","execution_count":null,"id":"2e65b1da","metadata":{"id":"2e65b1da"},"outputs":[],"source":["def run_feature_extraction():\n","    \"\"\"Orchestrate CLIP feature extraction and save to features.pkl.\"\"\"\n","    if not images_dir.exists():\n","        raise FileNotFoundError(f\"images/ directory not found at {images_dir}\")\n","\n","    image_files = list_image_files(images_dir)\n","    if not image_files:\n","        raise RuntimeError(f\"No image files found under {images_dir}\")\n","\n","    print(f\"\\nPreparing to extract features for {len(image_files):,} images.\")\n","\n","    model, preprocess = load_clip_model(CONFIG[\"model_name\"], device=device)\n","    print(\"\\nModel loaded.\")\n","    print(\"----------------\")\n","    print(model)\n","\n","    features, paths = process_all_images(\n","        image_files, model, preprocess, device, CONFIG[\"batch_size\"], normalize=CONFIG[\"normalize\"]\n","    )\n","\n","    print(f\"\\nFeature matrix shape: {features.shape}\")\n","    if features.size > 0:\n","        norms = np.linalg.norm(features[:5], axis=1)\n","        print(\"Sample norms (first 5 rows):\", [f\"{n:.3f}\" for n in norms])\n","\n","    data = {\n","        \"features\": features,\n","        \"paths\": paths,\n","        \"model_name\": CONFIG[\"model_name\"],\n","        \"normalized\": CONFIG[\"normalize\"],\n","        \"device\": device,\n","        \"n_images\": len(paths),\n","        \"feature_dim\": int(features.shape[1]) if features.size > 0 else 0,\n","        \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n","    }\n","\n","    output_path = Path(CONFIG[\"output_path\"])\n","    output_path.parent.mkdir(parents=True, exist_ok=True)\n","\n","    with open(output_path, \"wb\") as f:\n","        pickle.dump(data, f)\n","\n","    print(f\"\\nSaved features to: {output_path}\")\n","    return data"]},{"cell_type":"code","execution_count":null,"id":"a8bbad73","metadata":{"id":"a8bbad73"},"outputs":[],"source":["# Run feature extraction when executed as a script\n","if __name__ == \"__main__\":\n","    feature_data = run_feature_extraction()"]},{"cell_type":"code","execution_count":null,"id":"465c6f8e","metadata":{"id":"465c6f8e"},"outputs":[],"source":["# Verification / quick inspection cell\n","output_path = Path(CONFIG[\"output_path\"])\n","\n","if output_path.exists():\n","    with open(output_path, \"rb\") as f:\n","        loaded = pickle.load(f)\n","\n","    feats = np.asarray(loaded.get(\"features\"))\n","    paths = loaded.get(\"paths\", [])\n","\n","    print(\"\\nVerification\")\n","    print(\"------------\")\n","    print(f\"features shape : {feats.shape}\")\n","    print(f\"#paths         : {len(paths)}\")\n","\n","    if feats.size > 0:\n","        print(f\"mean: {feats.mean():.4f}, std: {feats.std():.4f}\")\n","        norms = np.linalg.norm(feats[:5], axis=1)\n","        print(\"sample norms   :\", [f\"{n:.3f}\" for n in norms])\n","\n","    print(\"\\nSample image paths:\")\n","    for p in paths[:3]:\n","        print(\" -\", p)\n","else:\n","    print(f\"\\nNo features file found at {output_path}\")"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}