{"cells":[{"cell_type":"markdown","id":"cb6224b6","metadata":{"id":"cb6224b6"},"source":["# Phase 1-1: YOLOv8 Finetuning\n","\n","Finetunes the last 16 layers of a yolov8"]},{"cell_type":"code","execution_count":null,"id":"3ad35ca0","metadata":{"id":"3ad35ca0"},"outputs":[],"source":["# Library imports\n","import os\n","import yaml\n","from pathlib import Path\n","from datetime import datetime\n","from collections import defaultdict\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","!pip install ultralytics -q\n","from ultralytics import YOLO\n","import torch\n","\n","# Google Drive mounting\n","try:\n","    from google.colab import drive  # type: ignore\n","    drive.mount('/content/drive')\n","except Exception:\n","    pass\n","\n","# Set style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.dpi'] = 100\n","\n","print(\"All packages imported successfully!\")\n","print(f\"Current directory: {os.getcwd()}\")\n","print(f\"GPU available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"code","execution_count":null,"id":"074b9ee7","metadata":{"id":"074b9ee7"},"outputs":[],"source":["# =============================================================================\n","# Environment Detection\n","# =============================================================================\n","\n","import sys\n","\n","# Determine if running in Colab or locally\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","print(\"Environment Detection:\")\n","print(\"=\" * 60)\n","\n","if IN_COLAB:\n","    print(\" Google Colab environment detected\")\n","else:\n","    print(\" Local environment detected\")"]},{"cell_type":"code","execution_count":null,"id":"8c99b230","metadata":{"id":"8c99b230"},"outputs":[],"source":["# =============================================================================\n","# Path Configuration - Update these for your computer\n","# =============================================================================\n","\n","# Google Drive paths (for Colab)\n","DRIVE_ROOT = \"/content/drive/MyDrive/Colab Notebooks/Data/production\"\n","\n","# Local machine paths\n","LOCAL_ROOT = \"/Users/tyreecruse/Desktop/CS230/Project/Data/production \"\n","\n","# Colab VM temporary storage\n","COLAB_LOCAL_ROOT = \"/content\"\n","\n","# Dataset configuration\n","ZIP_FILE_NAME = \"production.zip\"\n","ZIP_FILE_PATH_IN_DRIVE = os.path.join(DRIVE_ROOT, ZIP_FILE_NAME)\n","\n","print(\"\\nPath Configuration:\")\n","print(\"-\" * 40)\n","if IN_COLAB:\n","    print(f\"Drive root:  {DRIVE_ROOT}\")\n","    print(f\"Colab local: {COLAB_LOCAL_ROOT}\")\n","    print(f\"Zip file:    {ZIP_FILE_NAME}\")\n","else:\n","    print(f\"Local root:  {LOCAL_ROOT}\")"]},{"cell_type":"code","execution_count":null,"id":"da4754e4","metadata":{"id":"da4754e4"},"outputs":[],"source":["# =============================================================================\n","# Data Staging for Colab\n","# =============================================================================\n","\n","if IN_COLAB:\n","    print(\"\\nData Staging:\")\n","    print(\"-\" * 40)\n","\n","    # Define local paths in Colab VM\n","    local_zip_path = os.path.join(COLAB_LOCAL_ROOT, ZIP_FILE_NAME)\n","    local_dataset_path = os.path.join(COLAB_LOCAL_ROOT, \"production\")\n","\n","    # Only copy and unzip if dataset doesn't exist locally\n","    if not os.path.exists(local_dataset_path):\n","        print(f\"Step 1: Copying {ZIP_FILE_NAME} from Drive...\")\n","        !cp \"{ZIP_FILE_PATH_IN_DRIVE}\" \"{COLAB_LOCAL_ROOT}/\"\n","\n","        print(\"Step 2: Unzipping dataset...\")\n","        !unzip -q \"{local_zip_path}\" -d \"{COLAB_LOCAL_ROOT}\"\n","\n","        # Verify extraction\n","        if os.path.exists(local_dataset_path):\n","            file_count = len(os.listdir(local_dataset_path))\n","            print(f\" Data staging complete ({file_count} items)\")\n","        else:\n","            print(\" Error: Dataset extraction failed\")\n","    else:\n","        print(\" Dataset already staged locally\")\n","\n","    # Set paths for Colab\n","    PROJECT_ROOT = COLAB_LOCAL_ROOT  # Read from fast local disk\n","    OUTPUT_ROOT = DRIVE_ROOT          # Write results to permanent Drive\n","\n","else:\n","    # Set paths for local machine\n","    PROJECT_ROOT = LOCAL_ROOT\n","    OUTPUT_ROOT = LOCAL_ROOT\n","    print(\"\\nUsing local paths - no staging required\")"]},{"cell_type":"code","execution_count":null,"id":"2a1f6af9","metadata":{"id":"2a1f6af9"},"outputs":[],"source":["# =============================================================================\n","# Configuration - Update based on staging and actual data structure\n","# =============================================================================\n","\n","if IN_COLAB:\n","    # Use the locally staged data for fast reading\n","    DATASET_ROOT = os.path.join(COLAB_LOCAL_ROOT, \"production\")\n","\n","    # Output to Drive for permanent storage\n","    OUTPUT_BASE = \"/content/drive/MyDrive/CS230/Colab Notebooks/Data/training results\"\n","\n","    # Create output directory if it doesn't exist\n","    os.makedirs(OUTPUT_BASE, exist_ok=True)\n","\n","    # Create data.yaml dynamically since it needs absolute paths\n","    data_yaml_content = {\n","        'path': DATASET_ROOT,\n","        'train': 'train/images',\n","        'val': 'val/images',\n","        'test': 'test/images',\n","        'nc': 1,\n","        'names': ['tank']\n","    }\n","\n","    # Write data.yaml to local staging area\n","    data_yaml_path = os.path.join(DATASET_ROOT, 'data.yaml')\n","    with open(data_yaml_path, 'w') as f:\n","        yaml.dump(data_yaml_content, f)\n","\n","else:\n","    # Local machine paths\n","    DATASET_ROOT = LOCAL_ROOT\n","    OUTPUT_BASE = \"/Users/tyreecruse/Desktop/CS230/Project\"\n","    data_yaml_path = os.path.join(DATASET_ROOT, 'data.yaml')\n","\n","CONFIG = {\n","    # Dataset paths - using staged local data in Colab\n","    \"dataset_yaml\": data_yaml_path,\n","    \"dataset_path\": DATASET_ROOT,\n","\n","    # Output paths - save ALL results to Drive under training results folder\n","    \"output_path\": OUTPUT_BASE,  # Base directory for all outputs\n","    \"experiment_name\": \"yolov8x_tank_baseline\",\n","\n","    # Model configuration\n","    \"model_size\": \"yolov8x.pt\",  # 68.2M parameters\n","\n","    # Training parameters\n","    \"epochs\": 150,\n","    \"batch_size\": 8,  # Adjust based on GPU memory\n","    \"imgsz\": 640,\n","    \"patience\": 30,\n","    \"save_period\": 5,\n","\n","    # Optimizer settings\n","    \"optimizer\": \"AdamW\",\n","    \"lr0\": 0.001,\n","    \"lrf\": 0.01,\n","    \"momentum\": 0.937,\n","    \"weight_decay\": 0.0005,\n","    \"warmup_epochs\": 3.0,\n","\n","    # Loss weights - optimized for single class detection\n","    \"box_weight\": 7.5,  # Higher for precise localization\n","    \"cls_weight\": 0.5,  # Lower for single class\n","    \"dfl_weight\": 1.5,\n","\n","    # Augmentation - minimal for clean baseline\n","    \"hsv_h\": 0.015,\n","    \"hsv_s\": 0.4,\n","    \"hsv_v\": 0.2,\n","    \"degrees\": 5.0,\n","    \"translate\": 0.1,\n","    \"scale\": 0.3,\n","    \"mosaic\": 0.25,  # Minimal mosaic\n","    \"mixup\": 0.0,  # No mixup for baseline\n","\n","    # Monitoring\n","    \"test_eval_period\": 5,\n","    \"plot_results\": True,\n","    \"save_json\": True,\n","}\n","\n","# Display configuration\n","print(\"\\nConfiguration:\")\n","print(\"=\" * 60)\n","print(f\"Dataset YAML:    {CONFIG['dataset_yaml']}\")\n","print(f\"Dataset path:    {CONFIG['dataset_path']}\")\n","print(f\"Output base:     {CONFIG['output_path']}\")\n","print(f\"Experiment dir:  {os.path.join(CONFIG['output_path'], CONFIG['experiment_name'])}\")\n","print(f\"Model:           {CONFIG['model_size']}\")\n","print(f\"Epochs:          {CONFIG['epochs']}\")\n","print(f\"Batch size:      {CONFIG['batch_size']}\")\n","print(f\"Image size:      {CONFIG['imgsz']}\")\n","\n","# Verify dataset structure\n","if IN_COLAB:\n","    print(\"\\nDataset Structure Verification:\")\n","    print(\"-\" * 40)\n","    for split in ['train', 'val', 'test']:\n","        img_path = os.path.join(DATASET_ROOT, split, 'images')\n","        lbl_path = os.path.join(DATASET_ROOT, split, 'labels')\n","        if os.path.exists(img_path) and os.path.exists(lbl_path):\n","            img_count = len(os.listdir(img_path))\n","            lbl_count = len(os.listdir(lbl_path))\n","            print(f\"{split:5s}: {img_count:4d} images, {lbl_count:4d} labels\")\n","        else:\n","            print(f\"{split:5s}: NOT FOUND\")\n","\n","# Verify output directory\n","if os.path.exists(CONFIG[\"output_path\"]):\n","    print(f\"\\nOutput directory verified: {CONFIG['output_path']}\")\n","else:\n","    print(f\"\\nCreating output directory: {CONFIG['output_path']}\")\n","    os.makedirs(CONFIG[\"output_path\"], exist_ok=True)\n","\n","# Verify data.yaml exists\n","if os.path.exists(CONFIG[\"dataset_yaml\"]):\n","    print(f\"\\nDataset YAML created/found at: {CONFIG['dataset_yaml']}\")\n","    with open(CONFIG[\"dataset_yaml\"], 'r') as f:\n","        data_config = yaml.safe_load(f)\n","    print(f\"Classes: {data_config.get('nc', 'unknown')}\")\n","    print(f\"Names: {data_config.get('names', 'unknown')}\")\n","else:\n","    print(f\"\\nDataset YAML not found at: {CONFIG['dataset_yaml']}\")"]},{"cell_type":"code","execution_count":null,"id":"c59705f5","metadata":{"id":"c59705f5"},"outputs":[],"source":["def verify_dataset_structure(dataset_yaml_path):\n","    \"\"\"\n","    Verify dataset structure and paths.\n","\n","    Parameters\n","    ----------\n","    dataset_yaml_path : str or Path\n","        Path to dataset YAML file\n","\n","    Returns\n","    -------\n","    verification : dict\n","        Verification results\n","    \"\"\"\n","    yaml_path = Path(dataset_yaml_path)\n","\n","    verification = {\n","        'yaml_exists': False,\n","        'splits': {},\n","        'total_images': 0,\n","        'total_labels': 0\n","    }\n","\n","    if not yaml_path.exists():\n","        return verification\n","\n","    verification['yaml_exists'] = True\n","\n","    with open(yaml_path, 'r') as f:\n","        data_config = yaml.safe_load(f)\n","\n","    base_path = Path(data_config.get('path', yaml_path.parent))\n","\n","    for split in ['train', 'val', 'test']:\n","        split_path = data_config.get(split, '')\n","        if split_path:\n","            images_dir = base_path / split_path\n","            labels_dir = images_dir.parent / 'labels'\n","\n","            if images_dir.exists():\n","                image_count = len(list(images_dir.glob('*.jpg')) +\n","                                 list(images_dir.glob('*.png')))\n","                label_count = len(list(labels_dir.glob('*.txt'))) if labels_dir.exists() else 0\n","\n","                verification['splits'][split] = {\n","                    'images': image_count,\n","                    'labels': label_count,\n","                    'path': str(images_dir)\n","                }\n","\n","                verification['total_images'] += image_count\n","                verification['total_labels'] += label_count\n","\n","    return verification"]},{"cell_type":"code","execution_count":null,"id":"280a7d31","metadata":{"id":"280a7d31"},"outputs":[],"source":["def display_dataset_info(verification):\n","    \"\"\"\n","    Display dataset information from verification.\n","\n","    Parameters\n","    ----------\n","    verification : dict\n","        Verification results from verify_dataset_structure\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"DATASET STRUCTURE VERIFICATION\")\n","    print(\"=\"*60)\n","\n","    if not verification['yaml_exists']:\n","        print(\"Dataset YAML not found!\")\n","        return\n","\n","    print(f\"\\nTotal images: {verification['total_images']:,}\")\n","    print(f\"Total labels: {verification['total_labels']:,}\")\n","\n","    print(\"\\nSplit breakdown:\")\n","    print(\"-\"*40)\n","    for split, info in verification['splits'].items():\n","        coverage = info['labels'] / info['images'] * 100 if info['images'] > 0 else 0\n","        print(f\"{split:5s}: {info['images']:5,} images, {info['labels']:5,} labels ({coverage:.1f}% coverage)\")"]},{"cell_type":"code","execution_count":null,"id":"228f64db","metadata":{"id":"228f64db"},"outputs":[],"source":["def analyze_split_statistics(dataset_yaml_path):\n","    \"\"\"\n","    Analyze statistics for each split to detect outliers and imbalances.\n","\n","    Parameters\n","    ----------\n","    dataset_yaml_path : str or Path\n","        Path to dataset YAML file\n","\n","    Returns\n","    -------\n","    analysis : dict\n","        Statistics for each split\n","    \"\"\"\n","    yaml_path = Path(dataset_yaml_path)\n","\n","    with open(yaml_path, 'r') as f:\n","        data_config = yaml.safe_load(f)\n","\n","    base_path = Path(data_config.get('path', yaml_path.parent))\n","    analysis = {}\n","\n","    for split in ['train', 'val', 'test']:\n","        split_path = data_config.get(split, '')\n","        if split_path:\n","            labels_dir = base_path / split_path.replace('images', 'labels')\n","\n","            box_counts = []\n","            empty_files = 0\n","            dense_files = []\n","\n","            for label_file in labels_dir.glob('*.txt'):\n","                with open(label_file, 'r') as f:\n","                    lines = f.readlines()\n","\n","                num_boxes = len([l for l in lines if l.strip()])\n","                box_counts.append(num_boxes)\n","\n","                if num_boxes == 0:\n","                    empty_files += 1\n","                elif num_boxes > 10:  # Threshold for dense images\n","                    dense_files.append((label_file.name, num_boxes))\n","\n","            analysis[split] = {\n","                'total_images': len(box_counts),\n","                'avg_boxes': np.mean(box_counts) if box_counts else 0,\n","                'std_boxes': np.std(box_counts) if box_counts else 0,\n","                'min_boxes': min(box_counts) if box_counts else 0,\n","                'max_boxes': max(box_counts) if box_counts else 0,\n","                'empty_files': empty_files,\n","                'dense_files': len(dense_files),\n","                'box_distribution': box_counts\n","            }\n","\n","    return analysis"]},{"cell_type":"code","execution_count":null,"id":"f25a684b","metadata":{"id":"f25a684b"},"outputs":[],"source":["def compare_split_distributions(analysis):\n","    \"\"\"\n","    Compare distributions between splits to detect inconsistencies.\n","\n","    Parameters\n","    ----------\n","    analysis : dict\n","        Split statistics from analyze_split_statistics\n","\n","    Returns\n","    -------\n","    comparison : dict\n","        Comparison metrics between splits\n","    \"\"\"\n","    from scipy import stats\n","\n","    comparison = {}\n","\n","    # Compare val vs test distributions\n","    if 'val' in analysis and 'test' in analysis:\n","        val_dist = analysis['val']['box_distribution']\n","        test_dist = analysis['test']['box_distribution']\n","\n","        # Kolmogorov-Smirnov test\n","        ks_statistic, ks_pvalue = stats.ks_2samp(val_dist, test_dist)\n","\n","        comparison['val_vs_test'] = {\n","            'ks_statistic': ks_statistic,\n","            'ks_pvalue': ks_pvalue,\n","            'similar_distribution': ks_pvalue > 0.05,\n","            'avg_box_diff': abs(analysis['val']['avg_boxes'] - analysis['test']['avg_boxes'])\n","        }\n","\n","    # Compare train vs val\n","    if 'train' in analysis and 'val' in analysis:\n","        train_dist = analysis['train']['box_distribution']\n","        val_dist = analysis['val']['box_distribution']\n","\n","        ks_statistic, ks_pvalue = stats.ks_2samp(train_dist, val_dist)\n","\n","        comparison['train_vs_val'] = {\n","            'ks_statistic': ks_statistic,\n","            'ks_pvalue': ks_pvalue,\n","            'similar_distribution': ks_pvalue > 0.05,\n","            'avg_box_diff': abs(analysis['train']['avg_boxes'] - analysis['val']['avg_boxes'])\n","        }\n","\n","    return comparison"]},{"cell_type":"code","execution_count":null,"id":"fac8db65","metadata":{"id":"fac8db65"},"outputs":[],"source":["def plot_split_distributions(analysis):\n","    \"\"\"\n","    Visualize box count distributions across splits.\n","\n","    Parameters\n","    ----------\n","    analysis : dict\n","        Split statistics from analyze_split_statistics\n","    \"\"\"\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","    for idx, split in enumerate(['train', 'val', 'test']):\n","        if split in analysis:\n","            ax = axes[idx]\n","            box_counts = analysis[split]['box_distribution']\n","\n","            ax.hist(box_counts, bins=20, edgecolor='black', alpha=0.7)\n","            ax.axvline(analysis[split]['avg_boxes'], color='red',\n","                      linestyle='--', label=f'Mean: {analysis[split][\"avg_boxes\"]:.2f}')\n","\n","            ax.set_xlabel('Boxes per Image')\n","            ax.set_ylabel('Frequency')\n","            ax.set_title(f'{split.capitalize()} Split (n={len(box_counts)})')\n","            ax.legend()\n","            ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.suptitle('Box Count Distribution Across Splits', y=1.02)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1822e009","metadata":{"id":"1822e009"},"outputs":[],"source":["def create_training_args(config):\n","    \"\"\"\n","    Create training arguments dictionary from config.\n","\n","    Parameters\n","    ----------\n","    config : dict\n","        Configuration dictionary\n","\n","    Returns\n","    -------\n","    args : dict\n","        Training arguments for YOLO\n","    \"\"\"\n","    args = {\n","        'data': config['dataset_yaml'],\n","        'epochs': config['epochs'],\n","        'batch': config['batch_size'],\n","        'imgsz': config['imgsz'],\n","        'patience': config['patience'],\n","        'save': True,\n","        'save_period': config['save_period'],\n","        'device': 0,\n","        'workers': 2,\n","        'project': config['output_path'],\n","        'name': config['experiment_name'],\n","        'exist_ok': True,\n","        'pretrained': True,\n","        'optimizer': config['optimizer'],\n","        'lr0': config['lr0'],\n","        'lrf': config['lrf'],\n","        'momentum': config['momentum'],\n","        'weight_decay': config['weight_decay'],\n","        'warmup_epochs': config['warmup_epochs'],\n","        'box': config['box_weight'],\n","        'cls': config['cls_weight'],\n","        'dfl': config['dfl_weight'],\n","        'hsv_h': config['hsv_h'],\n","        'hsv_s': config['hsv_s'],\n","        'hsv_v': config['hsv_v'],\n","        'degrees': config['degrees'],\n","        'translate': config['translate'],\n","        'scale': config['scale'],\n","        'shear': 0.0,\n","        'perspective': 0.0,\n","        'flipud': 0.0,\n","        'fliplr': 0.5,\n","        'mosaic': config['mosaic'],\n","        'mixup': config['mixup'],\n","        'copy_paste': 0.0,\n","        'close_mosaic': 100,\n","        'amp': True,\n","        'val': True,\n","        'plots': config['plot_results'],\n","        'save_json': config['save_json']\n","    }\n","    return args"]},{"cell_type":"code","execution_count":null,"id":"9fa25d44","metadata":{"id":"9fa25d44"},"outputs":[],"source":["def create_test_evaluation_callback(config, test_results):\n","    \"\"\"\n","    Create callback for periodic test set evaluation.\n","\n","    Parameters\n","    ----------\n","    config : dict\n","        Configuration dictionary\n","    test_results : list\n","        List to store test results\n","\n","    Returns\n","    -------\n","    callback : function\n","        Callback function\n","    \"\"\"\n","    def evaluate_on_test(trainer):\n","        if trainer.epoch % config['test_eval_period'] == 0 or trainer.epoch == trainer.epochs - 1:\n","            model = YOLO(trainer.best)\n","            metrics = model.val(\n","                data=config['dataset_yaml'],\n","                split='test',\n","                batch=config['batch_size']\n","            )\n","\n","            # Note: Loss values are not available from validation metrics\n","            # We can only get performance metrics\n","            test_results.append({\n","                'epoch': trainer.epoch,\n","                'test_map50': float(metrics.box.map50),\n","                'test_map': float(metrics.box.map),\n","                'test_precision': float(metrics.box.p[0]) if len(metrics.box.p) > 0 else 0,\n","                'test_recall': float(metrics.box.r[0]) if len(metrics.box.r) > 0 else 0,\n","                'test_f1': float(metrics.box.f1[0]) if len(metrics.box.f1) > 0 else 0,\n","                # Calculate approximate total loss from trainer if available\n","                'test_box_loss': None,  # Not available from validation\n","                'test_cls_loss': None,  # Not available from validation\n","                'test_dfl_loss': None,  # Not available from validation\n","                'test_total_loss': None  # Not available from validation\n","            })\n","\n","            # Save intermediate results\n","            results_path = Path(config['output_path']) / config['experiment_name'] / 'test_metrics.csv'\n","            pd.DataFrame(test_results).to_csv(results_path, index=False)\n","\n","            print(f\"Epoch {trainer.epoch}: Test mAP50={metrics.box.map50:.3f}, mAP={metrics.box.map:.3f}\")\n","\n","    return evaluate_on_test"]},{"cell_type":"code","execution_count":null,"id":"88d7153b","metadata":{"id":"88d7153b"},"outputs":[],"source":["def save_training_config(config):\n","    \"\"\"\n","    Save training configuration to file.\n","\n","    Parameters\n","    ----------\n","    config : dict\n","        Configuration dictionary\n","    \"\"\"\n","    output_dir = Path(config['output_path']) / config['experiment_name']\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    config_path = output_dir / 'training_config.yaml'\n","    with open(config_path, 'w') as f:\n","        yaml.dump(config, f, default_flow_style=False)\n","\n","    print(f\"Configuration saved to: {config_path}\")"]},{"cell_type":"code","execution_count":null,"id":"d6da2833","metadata":{"id":"d6da2833"},"outputs":[],"source":["def train_yolov8x(config, test_results):\n","    \"\"\"\n","    Train YOLOv8x model with configuration.\n","\n","    Parameters\n","    ----------\n","    config : dict\n","        Configuration dictionary\n","    test_results : list\n","        List to store test results\n","\n","    Returns\n","    -------\n","    model : YOLO\n","        Trained model\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"STARTING YOLOV8X TRAINING\")\n","    print(\"=\"*60)\n","\n","    # Initialize model\n","    model = YOLO(config['model_size'])\n","    print(f\"Model loaded: {config['model_size']}\")\n","\n","    # Add test evaluation callback\n","    callback = create_test_evaluation_callback(config, test_results)\n","    model.add_callback(\"on_fit_epoch_end\", callback)\n","\n","    # Get training arguments\n","    args = create_training_args(config)\n","\n","    # Save configuration\n","    save_training_config(config)\n","\n","    # Train model\n","    print(\"\\nTraining started...\")\n","    print(\"-\"*60)\n","    results = model.train(**args)\n","\n","    print(\"-\"*60)\n","    print(\"Training completed!\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"218fac3d","metadata":{"id":"218fac3d"},"outputs":[],"source":["def load_training_metrics(experiment_path):\n","    \"\"\"\n","    Load training metrics from results files.\n","\n","    Parameters\n","    ----------\n","    experiment_path : str or Path\n","        Path to experiment directory\n","\n","    Returns\n","    -------\n","    metrics : dict\n","        Dictionary with training and test metrics\n","    \"\"\"\n","    exp_path = Path(experiment_path)\n","\n","    metrics = {}\n","\n","    # Load training/validation results\n","    results_path = exp_path / 'results.csv'\n","    if results_path.exists():\n","        metrics['train_val'] = pd.read_csv(results_path)\n","\n","    # Load test results\n","    test_path = exp_path / 'test_metrics.csv'\n","    if test_path.exists():\n","        metrics['test'] = pd.read_csv(test_path)\n","\n","    return metrics"]},{"cell_type":"code","execution_count":null,"id":"3d34e3f5","metadata":{"id":"3d34e3f5"},"outputs":[],"source":["def plot_loss_curves(metrics):\n","    \"\"\"\n","    Plot comprehensive loss curves.\n","\n","    Parameters\n","    ----------\n","    metrics : dict\n","        Dictionary with training metrics\n","\n","    Returns\n","    -------\n","    fig : matplotlib.figure\n","        Figure with plots\n","    \"\"\"\n","    if 'train_val' not in metrics:\n","        print(\"No training metrics found\")\n","        return None\n","\n","    df = metrics['train_val']\n","    test_df = metrics.get('test', pd.DataFrame())\n","\n","    # Calculate total losses\n","    df['train_total'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\n","    df['val_total'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\n","\n","    # Create figure\n","    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n","\n","    # Total loss\n","    ax = axes[0, 0]\n","    ax.plot(df.index, df['train_total'], label='Train', alpha=0.7)\n","    ax.plot(df.index, df['val_total'], label='Val', alpha=0.7)\n","    if not test_df.empty:\n","        test_total = test_df['test_box_loss'] + test_df['test_cls_loss'] + test_df['test_dfl_loss']\n","        ax.plot(test_df['epoch'], test_total, 'o-', label='Test', markersize=4)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Total Loss')\n","    ax.set_title('Total Loss Comparison')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # mAP performance\n","    ax = axes[0, 1]\n","    ax.plot(df.index, df['metrics/mAP50(B)'], label='Val mAP50', alpha=0.7)\n","    ax.plot(df.index, df['metrics/mAP50-95(B)'], label='Val mAP50-95', alpha=0.7)\n","    if not test_df.empty:\n","        ax.plot(test_df['epoch'], test_df['test_map50'], 'o-', label='Test mAP50', markersize=4)\n","        ax.plot(test_df['epoch'], test_df['test_map'], 's-', label='Test mAP50-95', markersize=4)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('mAP')\n","    ax.set_title('mAP Performance')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # Overfitting gap\n","    ax = axes[0, 2]\n","    gap = df['val_total'] - df['train_total']\n","    ax.plot(df.index, gap, color='red', alpha=0.7)\n","    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n","    ax.fill_between(df.index, 0, gap, where=(gap > 0), color='red', alpha=0.3)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Val - Train Loss')\n","    ax.set_title('Overfitting Detection')\n","    ax.grid(True, alpha=0.3)\n","\n","    # Component losses\n","    ax = axes[1, 0]\n","    ax.plot(df.index, df['train/box_loss'], label='Box', alpha=0.7)\n","    ax.plot(df.index, df['train/cls_loss'], label='Class', alpha=0.7)\n","    ax.plot(df.index, df['train/dfl_loss'], label='DFL', alpha=0.7)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Loss')\n","    ax.set_title('Training Loss Components')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # Precision/Recall\n","    ax = axes[1, 1]\n","    ax.plot(df.index, df['metrics/precision(B)'], label='Precision', alpha=0.7)\n","    ax.plot(df.index, df['metrics/recall(B)'], label='Recall', alpha=0.7)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Score')\n","    ax.set_title('Precision & Recall')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # Learning rate\n","    ax = axes[1, 2]\n","    ax.plot(df.index, df['lr/pg0'], alpha=0.7)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Learning Rate')\n","    ax.set_title('Learning Rate Schedule')\n","    ax.set_yscale('log')\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    return fig"]},{"cell_type":"code","execution_count":null,"id":"da5270b2","metadata":{"id":"da5270b2"},"outputs":[],"source":["def compare_val_test_performance(metrics):\n","    \"\"\"\n","    Compare validation and test set performance to detect distribution issues.\n","\n","    Parameters\n","    ----------\n","    metrics : dict\n","        Dictionary with training metrics\n","\n","    Returns\n","    -------\n","    comparison : dict\n","        Val vs Test comparison metrics\n","    \"\"\"\n","    if 'train_val' not in metrics or 'test' not in metrics:\n","        return None\n","\n","    train_val_df = metrics['train_val']\n","    test_df = metrics['test']\n","\n","    if test_df.empty:\n","        return None\n","\n","    comparison = {}\n","\n","    # Get validation metrics at test evaluation epochs\n","    for _, test_row in test_df.iterrows():\n","        epoch = int(test_row['epoch'])\n","\n","        if epoch < len(train_val_df):\n","            val_row = train_val_df.iloc[epoch]\n","\n","            # Calculate differences (only for available metrics)\n","            map50_diff = abs(val_row['metrics/mAP50(B)'] - test_row['test_map50'])\n","            map_diff = abs(val_row['metrics/mAP50-95(B)'] - test_row['test_map'])\n","\n","            comparison[epoch] = {\n","                'val_map50': val_row['metrics/mAP50(B)'],\n","                'test_map50': test_row['test_map50'],\n","                'map50_diff': map50_diff,\n","                'val_map': val_row['metrics/mAP50-95(B)'],\n","                'test_map': test_row['test_map'],\n","                'map_diff': map_diff,\n","                'performance_gap': (map50_diff / val_row['metrics/mAP50(B)']) * 100 if val_row['metrics/mAP50(B)'] > 0 else 0\n","            }\n","\n","    return comparison"]},{"cell_type":"code","execution_count":null,"id":"d3633503","metadata":{"id":"d3633503"},"outputs":[],"source":["def plot_val_test_comparison(comparison):\n","    \"\"\"\n","    Plot validation vs test performance comparison.\n","\n","    Parameters\n","    ----------\n","    comparison : dict\n","        Val vs Test comparison from compare_val_test_performance\n","    \"\"\"\n","    if not comparison:\n","        print(\"No val vs test comparison data available\")\n","        return\n","\n","    epochs = sorted(comparison.keys())\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n","\n","    # mAP50 comparison\n","    ax = axes[0, 0]\n","    val_map50 = [comparison[e]['val_map50'] for e in epochs]\n","    test_map50 = [comparison[e]['test_map50'] for e in epochs]\n","    ax.plot(epochs, val_map50, 'o-', label='Validation', markersize=6)\n","    ax.plot(epochs, test_map50, 's-', label='Test', markersize=6)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('mAP50')\n","    ax.set_title('Val vs Test mAP50')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # mAP50-95 comparison\n","    ax = axes[0, 1]\n","    val_map = [comparison[e]['val_map'] for e in epochs]\n","    test_map = [comparison[e]['test_map'] for e in epochs]\n","    ax.plot(epochs, val_map, 'o-', label='Validation', markersize=6)\n","    ax.plot(epochs, test_map, 's-', label='Test', markersize=6)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('mAP50-95')\n","    ax.set_title('Val vs Test mAP50-95')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    # Performance gap\n","    ax = axes[1, 0]\n","    map50_diff = [comparison[e]['map50_diff'] for e in epochs]\n","    ax.plot(epochs, map50_diff, 'o-', color='red', markersize=6)\n","    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('|Val - Test| mAP50')\n","    ax.set_title('Absolute mAP50 Difference')\n","    ax.grid(True, alpha=0.3)\n","\n","    # Performance gap percentage\n","    ax = axes[1, 1]\n","    perf_gap = [comparison[e]['performance_gap'] for e in epochs]\n","    ax.plot(epochs, perf_gap, 'o-', color='purple', markersize=6)\n","    ax.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='5% threshold')\n","    ax.axhline(y=10, color='red', linestyle='--', alpha=0.5, label='10% threshold')\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Performance Gap (%)')\n","    ax.set_title('Val-Test Performance Gap')\n","    ax.legend()\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c308b1ea","metadata":{"id":"c308b1ea"},"outputs":[],"source":["def diagnose_val_test_consistency(comparison):\n","    \"\"\"\n","    Diagnose if validation and test sets are consistently distributed.\n","\n","    Parameters\n","    ----------\n","    comparison : dict\n","        Val vs Test comparison metrics\n","\n","    Returns\n","    -------\n","    diagnosis : dict\n","        Diagnosis of val/test consistency\n","    \"\"\"\n","    if not comparison:\n","        return {'status': 'no_data', 'message': 'No comparison data available'}\n","\n","    # Calculate average differences\n","    epochs = list(comparison.keys())\n","    avg_map_diff = np.mean([comparison[e]['map50_diff'] for e in epochs])\n","    # REMOVED: avg_loss_diff line - this key doesn't exist in comparison dict\n","    avg_perf_gap = np.mean([comparison[e]['performance_gap'] for e in epochs])\n","\n","    diagnosis = {\n","        'avg_map_difference': avg_map_diff,\n","        # REMOVED: 'avg_loss_difference' key\n","        'avg_performance_gap': avg_perf_gap,\n","        'issues': [],\n","        'recommendations': []\n","    }\n","\n","    # Determine status\n","    if avg_perf_gap < 5:\n","        diagnosis['status'] = 'excellent'\n","        diagnosis['message'] = 'Validation and test sets are well-aligned'\n","    elif avg_perf_gap < 10:\n","        diagnosis['status'] = 'good'\n","        diagnosis['message'] = 'Minor differences between validation and test sets'\n","    else:\n","        diagnosis['status'] = 'concerning'\n","        diagnosis['message'] = 'Significant differences between validation and test sets'\n","        diagnosis['issues'].append(f'Performance gap of {avg_perf_gap:.1f}% detected')\n","        diagnosis['recommendations'].append('Consider re-splitting data for better distribution')\n","        diagnosis['recommendations'].append('Check for data leakage or distribution shift')\n","\n","    return diagnosis"]},{"cell_type":"code","execution_count":null,"id":"8715e67a","metadata":{"id":"8715e67a"},"outputs":[],"source":["def generate_confusion_matrix_for_split(model_path, data_yaml, split='val'):\n","    \"\"\"\n","    Generate confusion matrix for a specific split.\n","\n","    Parameters\n","    ----------\n","    model_path : str or Path\n","        Path to trained model\n","    data_yaml : str or Path\n","        Path to data configuration\n","    split : str\n","        Split to evaluate ('train', 'val', 'test')\n","\n","    Returns\n","    -------\n","    results : dict\n","        Confusion matrix and metrics\n","    \"\"\"\n","    model = YOLO(model_path)\n","\n","    # Run validation on specified split\n","    metrics = model.val(data=str(data_yaml), split=split, save_json=True)\n","\n","    results = {\n","        'split': split,\n","        'map50': float(metrics.box.map50),\n","        'map50_95': float(metrics.box.map),\n","        'precision': float(metrics.box.p[0]) if len(metrics.box.p) > 0 else 0,\n","        'recall': float(metrics.box.r[0]) if len(metrics.box.r) > 0 else 0,\n","        'confusion_matrix': metrics.confusion_matrix.matrix if hasattr(metrics, 'confusion_matrix') else None\n","    }\n","\n","    # Calculate F1 score\n","    if results['precision'] > 0 and results['recall'] > 0:\n","        results['f1'] = 2 * (results['precision'] * results['recall']) / \\\n","                       (results['precision'] + results['recall'])\n","    else:\n","        results['f1'] = 0\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"id":"b1d85020","metadata":{"id":"b1d85020"},"outputs":[],"source":["def compare_split_performance(model_path, data_yaml):\n","    \"\"\"\n","    Compare performance across all splits.\n","\n","    Parameters\n","    ----------\n","    model_path : str or Path\n","        Path to trained model\n","    data_yaml : str or Path\n","        Path to data configuration\n","\n","    Returns\n","    -------\n","    comparison : dict\n","        Performance comparison across splits\n","    \"\"\"\n","    comparison = {}\n","\n","    for split in ['train', 'val', 'test']:\n","        print(f\"Evaluating {split} split...\")\n","        comparison[split] = generate_confusion_matrix_for_split(\n","            model_path, data_yaml, split\n","        )\n","\n","    return comparison"]},{"cell_type":"code","execution_count":null,"id":"a71c6137","metadata":{"id":"a71c6137"},"outputs":[],"source":["def plot_split_performance_comparison(comparison):\n","    \"\"\"\n","    Visualize performance comparison across splits.\n","\n","    Parameters\n","    ----------\n","    comparison : dict\n","        Performance metrics for each split\n","    \"\"\"\n","    splits = list(comparison.keys())\n","    metrics_names = ['map50', 'map50_95', 'precision', 'recall', 'f1']\n","\n","    # Prepare data for plotting\n","    data = {metric: [] for metric in metrics_names}\n","    for split in splits:\n","        for metric in metrics_names:\n","            data[metric].append(comparison[split].get(metric, 0))\n","\n","    # Create bar plot\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","\n","    x = np.arange(len(splits))\n","    width = 0.15\n","\n","    for i, metric in enumerate(metrics_names):\n","        offset = width * (i - 2)\n","        ax.bar(x + offset, data[metric], width, label=metric.upper())\n","\n","    ax.set_xlabel('Split')\n","    ax.set_ylabel('Score')\n","    ax.set_title('Performance Comparison Across Splits')\n","    ax.set_xticks(x)\n","    ax.set_xticklabels([s.capitalize() for s in splits])\n","    ax.legend()\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Print detailed comparison\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"DETAILED SPLIT COMPARISON\")\n","    print(\"=\"*60)\n","\n","    for metric in metrics_names:\n","        print(f\"\\n{metric.upper()}:\")\n","        for split in splits:\n","            value = comparison[split].get(metric, 0)\n","            print(f\"  {split:5s}: {value:.3f}\")\n","\n","        # Calculate variance\n","        values = [comparison[split].get(metric, 0) for split in splits]\n","        if len(values) > 1:\n","            variance = np.std(values)\n","            print(f\"  Std Dev: {variance:.3f}\")"]},{"cell_type":"code","execution_count":null,"id":"90e5ab35","metadata":{"id":"90e5ab35"},"outputs":[],"source":["def analyze_training_health(metrics):\n","    \"\"\"\n","    Analyze training health and detect issues.\n","\n","    Parameters\n","    ----------\n","    metrics : dict\n","        Dictionary with training metrics\n","\n","    Returns\n","    -------\n","    diagnosis : dict\n","        Training diagnosis\n","    \"\"\"\n","    if 'train_val' not in metrics:\n","        return {'status': 'error', 'message': 'No metrics found'}\n","\n","    df = metrics['train_val']\n","\n","    # Calculate total losses\n","    df['train_total'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\n","    df['val_total'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\n","\n","    # Analyze last 20% of training\n","    window = max(1, int(len(df) * 0.2))\n","\n","    recent_stats = {\n","        'train_loss': df['train_total'].iloc[-window:].mean(),\n","        'val_loss': df['val_total'].iloc[-window:].mean(),\n","        'map50': df['metrics/mAP50(B)'].iloc[-window:].mean(),\n","        'map50_95': df['metrics/mAP50-95(B)'].iloc[-window:].mean(),\n","        'best_map50': df['metrics/mAP50(B)'].max(),\n","        'best_map50_95': df['metrics/mAP50-95(B)'].max(),\n","    }\n","\n","    recent_stats['val_train_gap'] = (\n","        (recent_stats['val_loss'] - recent_stats['train_loss']) /\n","        recent_stats['train_loss'] * 100\n","    )\n","\n","    # Diagnose\n","    diagnosis = {\n","        'status': 'healthy',\n","        'issues': [],\n","        'recommendations': [],\n","        'metrics': recent_stats\n","    }\n","\n","    # Check overfitting\n","    if recent_stats['val_train_gap'] > 20:\n","        diagnosis['status'] = 'overfitting'\n","        diagnosis['issues'].append(f\"Val loss {recent_stats['val_train_gap']:.1f}% higher than train\")\n","        diagnosis['recommendations'].append(\"Consider more augmentation or regularization\")\n","\n","    # Check performance\n","    if recent_stats['map50'] < 0.7:\n","        diagnosis['status'] = 'underperforming' if diagnosis['status'] == 'healthy' else diagnosis['status']\n","        diagnosis['issues'].append(f\"Low mAP50: {recent_stats['map50']:.3f}\")\n","        diagnosis['recommendations'].append(\"Consider training longer or adjusting hyperparameters\")\n","\n","    # Check convergence\n","    recent_gradient = np.gradient(df['val_total'].iloc[-window:])\n","    if abs(recent_gradient.mean()) < 0.001:\n","        diagnosis['issues'].append(\"Model has converged\")\n","        diagnosis['recommendations'].append(\"Consider early stopping\")\n","\n","    return diagnosis"]},{"cell_type":"code","execution_count":null,"id":"c60a3640","metadata":{"id":"c60a3640"},"outputs":[],"source":["def print_diagnosis(diagnosis):\n","    \"\"\"\n","    Print training diagnosis in formatted way.\n","\n","    Parameters\n","    ----------\n","    diagnosis : dict\n","        Diagnosis from analyze_training_health\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"TRAINING DIAGNOSIS\")\n","    print(\"=\"*60)\n","\n","    print(f\"\\nStatus: {diagnosis['status'].upper()}\")\n","\n","    if diagnosis['issues']:\n","        print(\"\\nIssues detected:\")\n","        for issue in diagnosis['issues']:\n","            print(f\"  - {issue}\")\n","    else:\n","        print(\"\\nNo issues detected\")\n","\n","    if diagnosis['recommendations']:\n","        print(\"\\nRecommendations:\")\n","        for rec in diagnosis['recommendations']:\n","            print(f\"  - {rec}\")\n","\n","    print(\"\\nMetrics (last 20% of training):\")\n","    print(\"-\"*40)\n","    metrics = diagnosis['metrics']\n","    print(f\"Train loss:      {metrics['train_loss']:.4f}\")\n","    print(f\"Val loss:        {metrics['val_loss']:.4f}\")\n","    print(f\"Val-Train gap:   {metrics['val_train_gap']:.1f}%\")\n","    print(f\"Current mAP50:   {metrics['map50']:.3f}\")\n","    print(f\"Best mAP50:      {metrics['best_map50']:.3f}\")\n","    print(f\"Current mAP50-95: {metrics['map50_95']:.3f}\")\n","    print(f\"Best mAP50-95:   {metrics['best_map50_95']:.3f}\")"]},{"cell_type":"code","execution_count":null,"id":"dc57dce8","metadata":{"id":"dc57dce8"},"outputs":[],"source":["# Cell 12a: Initialize and verify dataset structure\n","print(\"\\n\" + \"=\"*80)\n","print(\"YOLOV8X TANK DETECTION TRAINING\")\n","print(\"=\"*80)\n","\n","results = {}\n","\n","print(\"\\nStep 1: Verifying dataset structure...\")\n","verification = verify_dataset_structure(CONFIG['dataset_yaml'])\n","display_dataset_info(verification)\n","\n","if not verification['yaml_exists']:\n","    raise FileNotFoundError(f\"Dataset YAML not found: {CONFIG['dataset_yaml']}\")\n","\n","results['dataset_info'] = verification\n","print(\" Dataset verification complete\")"]},{"cell_type":"code","execution_count":null,"id":"0f087f81","metadata":{"id":"0f087f81"},"outputs":[],"source":["# Cell 3: Analyze Data Quality\n","print(\"\\nStep 2: Analyzing data quality across splits...\")\n","split_analysis = analyze_split_statistics(CONFIG['dataset_yaml'])\n","\n","print(\"\\nSplit Statistics:\")\n","print(\"-\"*40)\n","for split, stats in split_analysis.items():\n","    print(f\"{split:5s}: {stats['total_images']:4d} images, \"\n","          f\"avg {stats['avg_boxes']:.2f} boxes/image, \"\n","          f\"{stats['dense_files']} dense files\")\n","\n","# Compare distributions\n","distribution_comparison = compare_split_distributions(split_analysis)\n","\n","if 'val_vs_test' in distribution_comparison:\n","    val_test = distribution_comparison['val_vs_test']\n","    print(f\"\\nVal vs Test distribution similarity: \"\n","          f\"{'SIMILAR' if val_test['similar_distribution'] else 'DIFFERENT'} \"\n","          f\"(p-value: {val_test['ks_pvalue']:.3f})\")\n","\n","# Plot distributions\n","plot_split_distributions(split_analysis)\n","\n","results['split_analysis'] = split_analysis\n","results['distribution_comparison'] = distribution_comparison\n","print(\" Data quality analysis complete\")"]},{"cell_type":"code","execution_count":null,"id":"8615fff0","metadata":{"id":"8615fff0"},"outputs":[],"source":["# Cell 4: Train Model\n","print(\"\\nStep 3: Training YOLOv8x model...\")\n","print(\"-\"*60)\n","print(\"This step will take significant time...\")\n","print(\"-\"*60)\n","\n","test_results = []\n","model = train_yolov8x(CONFIG, test_results)\n","results['test_metrics'] = test_results\n","print(\" Model training complete\")"]},{"cell_type":"code","execution_count":null,"id":"fa0ee33d","metadata":{"id":"fa0ee33d"},"outputs":[],"source":["# Cell 5: Step 4 - Analyze Training Results\n","print(\"\\nStep 4: Analyzing training results...\")\n","experiment_path = Path(CONFIG['output_path']) / CONFIG['experiment_name']\n","\n","metrics = load_training_metrics(experiment_path)\n","results['metrics'] = metrics\n","\n","# Plot losses\n","fig = plot_loss_curves(metrics)\n","if fig:\n","    plot_path = experiment_path / 'loss_analysis.png'\n","    fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n","    plt.show()\n","    print(f\"Loss plots saved to: {plot_path}\")\n","\n","print(\" Training results analysis complete\")"]},{"cell_type":"code","execution_count":null,"id":"841f0395","metadata":{"id":"841f0395"},"outputs":[],"source":["# Cell 6: Step 5 - Compare Validation vs Test\n","print(\"\\nStep 5: Comparing validation vs test performance...\")\n","val_test_comparison = compare_val_test_performance(metrics)\n","\n","if val_test_comparison:\n","    plot_val_test_comparison(val_test_comparison)\n","    val_test_diagnosis = diagnose_val_test_consistency(val_test_comparison)\n","\n","    print(\"\\n\" + \"-\"*40)\n","    print(f\"Val/Test Consistency: {val_test_diagnosis['status'].upper()}\")\n","    print(f\"{val_test_diagnosis['message']}\")\n","    if val_test_diagnosis.get('avg_performance_gap'):\n","        print(f\"Average performance gap: {val_test_diagnosis['avg_performance_gap']:.2f}%\")\n","\n","    results['val_test_comparison'] = val_test_comparison\n","    results['val_test_diagnosis'] = val_test_diagnosis\n","    print(\" Val/Test comparison complete\")\n","else:\n","    print(\"No val/test comparison data available\")"]},{"cell_type":"code","execution_count":null,"id":"a1b0fc38","metadata":{"id":"a1b0fc38"},"outputs":[],"source":["# Cell 7: Step 6 - Confusion Matrix Analysis\n","print(\"\\nStep 6: Generating confusion matrices for all splits...\")\n","experiment_path = Path(CONFIG['output_path']) / CONFIG['experiment_name']\n","best_model = experiment_path / 'weights' / 'best.pt'\n","\n","if best_model.exists():\n","    print(f\"Using model: {best_model}\")\n","    split_comparison = compare_split_performance(best_model, CONFIG['dataset_yaml'])\n","    plot_split_performance_comparison(split_comparison)\n","    results['split_performance'] = split_comparison\n","    print(\" Confusion matrix analysis complete\")\n","else:\n","    print(f\"Best model not found at: {best_model}\")\n","    print(\"Skipping confusion matrix analysis\")"]},{"cell_type":"code","execution_count":null,"id":"180212f8","metadata":{"id":"180212f8"},"outputs":[],"source":["# Cell 8: Step 7 - Overall Training Diagnosis\n","print(\"\\nStep 7: Overall training diagnosis...\")\n","diagnosis = analyze_training_health(metrics)\n","results['diagnosis'] = diagnosis\n","print_diagnosis(diagnosis)\n","print(\" Training diagnosis complete\")"]},{"cell_type":"code","execution_count":null,"id":"1773c6e0","metadata":{"id":"1773c6e0"},"outputs":[],"source":["# Cell 9: Final Summary\n","print(\"\\n\" + \"=\"*80)\n","print(\"TRAINING PIPELINE SUMMARY\")\n","print(\"=\"*80)\n","\n","# Check what was completed\n","completed_steps = []\n","if 'dataset_info' in results:\n","    completed_steps.append(\"Dataset Verification\")\n","if 'split_analysis' in results:\n","    completed_steps.append(\"Data Quality Analysis\")\n","if 'test_metrics' in results:\n","    completed_steps.append(\"Model Training\")\n","if 'metrics' in results:\n","    completed_steps.append(\"Results Analysis\")\n","if 'val_test_comparison' in results:\n","    completed_steps.append(\"Val/Test Comparison\")\n","if 'split_performance' in results:\n","    completed_steps.append(\"Confusion Matrix Analysis\")\n","if 'diagnosis' in results:\n","    completed_steps.append(\"Training Diagnosis\")\n","\n","print(\"\\nCompleted Steps:\")\n","for i, step in enumerate(completed_steps, 1):\n","    print(f\"  {i}. {step}\")\n","\n","if 'error' not in results:\n","    print(\"\\n ALL STEPS COMPLETED SUCCESSFULLY\")\n","else:\n","    print(f\"\\n Pipeline failed with error: {results['error']}\")\n","\n","print(\"\\nResults dictionary contains:\", list(results.keys()))"]},{"cell_type":"code","execution_count":null,"id":"61eb1f66","metadata":{"id":"61eb1f66"},"outputs":[],"source":["# Cell 10: Save Results Summary to File (Optional)\n","import json\n","from datetime import datetime\n","\n","# Save results summary to Drive\n","summary_path = Path(CONFIG['output_path']) / CONFIG['experiment_name'] / 'training_summary.json'\n","\n","# Convert non-serializable objects to strings\n","results_summary = {\n","    'timestamp': datetime.now().isoformat(),\n","    'config': CONFIG,\n","    'dataset_info': results.get('dataset_info', {}),\n","    'distribution_comparison': results.get('distribution_comparison', {}),\n","    'val_test_diagnosis': results.get('val_test_diagnosis', {}),\n","    'final_diagnosis': results.get('diagnosis', {})\n","}\n","\n","with open(summary_path, 'w') as f:\n","    json.dump(results_summary, f, indent=2, default=str)\n","\n","print(f\"Training summary saved to: {summary_path}\")"]},{"cell_type":"code","execution_count":null,"id":"957290b4","metadata":{"id":"957290b4"},"outputs":[],"source":["print(\"\\n\" + \"=\"*60)\n","print(\"RESULTS VERIFICATION\")\n","print(\"=\"*60)\n","\n","experiment_path = Path(CONFIG['output_path']) / CONFIG['experiment_name']\n","\n","# Check model weights\n","weights_dir = experiment_path / 'weights'\n","if weights_dir.exists():\n","    weights = list(weights_dir.glob('*.pt'))\n","    print(f\"\\nModel weights: {len(weights)} files\")\n","    for weight_file in weights:\n","        size_mb = weight_file.stat().st_size / (1024 * 1024)\n","        print(f\"  - {weight_file.name}: {size_mb:.1f} MB\")\n","else:\n","    print(\"\\nNo weights directory found\")\n","\n","# Check output files\n","expected_files = [\n","    'results.csv',\n","    'results.png',\n","    'confusion_matrix.png',\n","    'test_metrics.csv',\n","    'training_config.yaml'\n","]\n","\n","print(\"\\nOutput files:\")\n","for filename in expected_files:\n","    filepath = experiment_path / filename\n","    if filepath.exists():\n","        size_kb = filepath.stat().st_size / 1024\n","        print(f\"  [{filename}]: {size_kb:.1f} KB\")\n","    else:\n","        print(f\"  [{filename}]: not found\")\n","\n","# Best model path\n","best_model = weights_dir / 'best.pt' if weights_dir else None\n","if best_model and best_model.exists():\n","    print(f\"\\nBest model path:\")\n","    print(f\"  {best_model}\")"]},{"cell_type":"code","execution_count":null,"id":"454286de","metadata":{"id":"454286de"},"outputs":[],"source":["print(\"\\n\" + \"=\"*60)\n","print(\"SUMMARY\")\n","print(\"=\"*60)\n","\n","if 'results' in locals():\n","    # Dataset info\n","    if 'dataset_info' in results:\n","        info = results['dataset_info']\n","        print(f\"\\nDataset:\")\n","        print(f\"  Total images: {info['total_images']:,}\")\n","        print(f\"  Total labels: {info['total_labels']:,}\")\n","        for split, data in info['splits'].items():\n","            print(f\"  {split}: {data['images']:,} images\")\n","\n","    # Data quality analysis\n","    if 'distribution_comparison' in results:\n","        comp = results['distribution_comparison']\n","        if 'val_vs_test' in comp:\n","            print(f\"\\nData Quality:\")\n","            val_test = comp['val_vs_test']\n","            print(f\"  Val/Test similarity: {'Similar' if val_test['similar_distribution'] else 'Different'}\")\n","            print(f\"  KS p-value: {val_test['ks_pvalue']:.3f}\")\n","\n","    # Val vs Test consistency\n","    if 'val_test_diagnosis' in results:\n","        diag = results['val_test_diagnosis']\n","        print(f\"\\nVal/Test Consistency:\")\n","        print(f\"  Status: {diag['status'].upper()}\")\n","        if 'avg_performance_gap' in diag:\n","            print(f\"  Average gap: {diag['avg_performance_gap']:.2f}%\")\n","\n","    # Split performance\n","    if 'split_performance' in results:\n","        perf = results['split_performance']\n","        print(f\"\\nPerformance Across Splits:\")\n","        for split in ['train', 'val', 'test']:\n","            if split in perf:\n","                print(f\"  {split:5s}: mAP50={perf[split]['map50']:.3f}, \"\n","                      f\"F1={perf[split]['f1']:.3f}\")\n","\n","    # Training results\n","    if 'diagnosis' in results:\n","        diagnosis = results['diagnosis']\n","        metrics = diagnosis['metrics']\n","        print(f\"\\nTraining Performance:\")\n","        print(f\"  Status: {diagnosis['status'].upper()}\")\n","        print(f\"  Best mAP50: {metrics['best_map50']:.3f}\")\n","        print(f\"  Best mAP50-95: {metrics['best_map50_95']:.3f}\")\n","        print(f\"  Final mAP50: {metrics['map50']:.3f}\")\n","        print(f\"  Val-Train gap: {metrics['val_train_gap']:.1f}%\")\n","\n","print(f\"\\nModel location: {CONFIG['output_path']}/{CONFIG['experiment_name']}/weights/best.pt\")"]},{"cell_type":"markdown","id":"89663bbd","metadata":{"id":"89663bbd"},"source":["# YOLOv8x Tank Detection - Baseline Training on Cleaned Dataset\n","\n","This notebook trains YOLOv8x on the pre-cleaned and clustered tank dataset found in the production folder"]},{"cell_type":"markdown","id":"032682f9","metadata":{"id":"032682f9"},"source":["## Table of Contents\n","- [1 - Packages](#1)\n","- [2 - Configuration](#2)\n","- [3 - Dataset Verification Functions](#3)\n","- [4 - Data Quality Analysis Functions](#4)\n","- [5 - Training Setup Functions](#5)\n","- [6 - Loss Monitoring Functions](#6)\n","- [7 - Training Execution Functions](#7)\n","- [8 - Performance Analysis Functions](#8)\n","- [9 - Validation vs Test Comparison Functions](#9)\n","- [10 - Confusion Matrix Analysis Functions](#10)\n","- [11 - Diagnostic Functions](#11)\n","- [12 - Main Execution](#12)\n","- [13 - Results Verification](#13)\n","- [14 - Summary](#14)"]},{"cell_type":"markdown","id":"2d00af26","metadata":{"id":"2d00af26"},"source":["<a name='1'></a>\n","## 1 - Packages"]},{"cell_type":"markdown","id":"3ecd7594","metadata":{"id":"3ecd7594"},"source":["<a name='2'></a>\n","## 2 - Configuration"]},{"cell_type":"markdown","id":"ac20a9d3","metadata":{"id":"ac20a9d3"},"source":["<a name='3'></a>\n","## 3 - Dataset Verification Functions"]},{"cell_type":"markdown","id":"40f7a7d8","metadata":{"id":"40f7a7d8"},"source":["<a name='4'></a>\n","## 4 - Data Quality Analysis Functions"]},{"cell_type":"markdown","id":"8312ecf7","metadata":{"id":"8312ecf7"},"source":["<a name='5'></a>\n","## 5 - Training Setup Functions"]},{"cell_type":"markdown","id":"e9bab6b0","metadata":{"id":"e9bab6b0"},"source":["<a name='5'></a>\n","## 5 - Loss Monitoring Functions"]},{"cell_type":"markdown","id":"c14d0384","metadata":{"id":"c14d0384"},"source":["<a name='6'></a>\n","## 6 - Training Execution Functions"]},{"cell_type":"markdown","id":"10e49046","metadata":{"id":"10e49046"},"source":["<a name='7'></a>\n","## 7 - Performance Analysis Functions"]},{"cell_type":"markdown","id":"bfd799b6","metadata":{"id":"bfd799b6"},"source":["<a name='8'></a>\n","## 8 - Validation vs Test Comparison Functions"]},{"cell_type":"markdown","id":"29ea6452","metadata":{"id":"29ea6452"},"source":["<a name='9'></a>\n","## 9 - Confusion Matrix Analysis Functions"]},{"cell_type":"markdown","id":"2c97eb99","metadata":{"id":"2c97eb99"},"source":["<a name='10'></a>\n","## 10 - Diagnostic Functions"]},{"cell_type":"markdown","id":"2f8c0fe9","metadata":{"id":"2f8c0fe9"},"source":["<a name='12'></a>\n","## 12 - Main Execution"]},{"cell_type":"markdown","id":"748edbe5","metadata":{"id":"748edbe5"},"source":["<a name='13'></a>\n","## 13 - Results Verification"]},{"cell_type":"markdown","id":"0c1c57fc","metadata":{"id":"0c1c57fc"},"source":["<a name='14'></a>\n","## 14 - Summary"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}