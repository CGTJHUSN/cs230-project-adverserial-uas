{"cells":[{"cell_type":"markdown","id":"2eaf35f8","metadata":{"id":"2eaf35f8"},"source":["# Phase 0-2: Dataset Cleaning\n","\n","Validate labels, find orphans, and remove unfixable files."]},{"cell_type":"code","execution_count":null,"id":"18937316","metadata":{"id":"18937316"},"outputs":[],"source":["import os\n","import shutil\n","from pathlib import Path\n","from datetime import datetime\n","\n","from tqdm import tqdm\n","\n","print(f\"Imports OK. CWD: {Path().resolve()}\")"]},{"cell_type":"code","execution_count":null,"id":"be985562","metadata":{"id":"be985562"},"outputs":[],"source":["# Paths are machine-specific. Adjust PROJECT_ROOT before running on a new machine.\n","PROJECT_ROOT = \"/Users/tyreecruse/Desktop/CS230/Project/Data/Original\"\n","# Or just use the current directory:\n","# PROJECT_ROOT = os.getcwd()\n","\n","CONFIG = {\n","    \"dataset_path\": os.path.join(PROJECT_ROOT, \"master_dataset_pool\"),\n","    \"remove_options\": {\n","        \"remove_empty_labels\": True,\n","        \"remove_wrong_format\": True,\n","        \"remove_invalid_format\": True,\n","        \"remove_orphan_images\": True,\n","        \"remove_orphan_labels\": True,\n","        \"remove_corresponding_files\": True,\n","    },\n","    \"create_backup\": False,\n","    \"backup_dir\": os.path.join(PROJECT_ROOT, \"dataset_backups\"),\n","}\n","\n","print(\"Config\")\n","print(\"------\")\n","print(f\"dataset_path: {CONFIG['dataset_path']}\")\n","print(\"removal options:\")\n","for key, enabled in CONFIG[\"remove_options\"].items():\n","    flag = \"yes\" if enabled else \"no\"\n","    print(f\"  {flag:3} {key.replace('_', ' ')}\")\n","\n","dataset_path = Path(CONFIG[\"dataset_path\"])\n","images_dir = dataset_path / \"images\"\n","labels_dir = dataset_path / \"labels\"\n","\n","if dataset_path.exists():\n","    n_img = len(list(images_dir.glob('*'))) if images_dir.exists() else 0\n","    n_lbl = len(list(labels_dir.glob('*.txt'))) if labels_dir.exists() else 0\n","    print(f\"\\nDataset found at {dataset_path}\")\n","    print(f\"  images: {n_img:,}\")\n","    print(f\"  labels: {n_lbl:,}\")\n","else:\n","    print(f\"\\n[warning] dataset not found at {dataset_path}\")"]},{"cell_type":"code","execution_count":null,"id":"a27b8ef9","metadata":{"id":"a27b8ef9"},"outputs":[],"source":["def create_backup(dataset_path, backup_dir):\n","    \"\"\"Copy dataset_path into backup_dir/<name>_backup_precleaning_<timestamp> and return the new path.\"\"\"\n","    dataset_path = Path(dataset_path)\n","    backup_dir = Path(backup_dir)\n","    backup_dir.mkdir(parents=True, exist_ok=True)\n","\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    backup_path = backup_dir / f\"{dataset_path.name}_backup_precleaning_{ts}\"\n","\n","    print(f\"\\nCreating backup at {backup_path}\")\n","    shutil.copytree(dataset_path, backup_path)\n","\n","    return backup_path"]},{"cell_type":"code","execution_count":null,"id":"0392884e","metadata":{"id":"0392884e"},"outputs":[],"source":["def validate_label_file(label_path):\n","    \"\"\"Return (is_valid, reason) for a YOLO-style label file.\"\"\"\n","    label_path = Path(label_path)\n","    try:\n","        text = label_path.read_text().strip()\n","    except Exception as exc:\n","        return False, f\"error: {exc}\"\n","\n","    if not text:\n","        return False, \"empty_file\"\n","\n","    for line in text.splitlines():\n","        line = line.strip()\n","        if not line:\n","            continue\n","\n","        parts = line.split()\n","        if len(parts) != 5:\n","            return False, \"wrong_format\"\n","\n","        try:\n","            class_id = int(parts[0])\n","            x, y, w, h = map(float, parts[1:])\n","        except ValueError:\n","            return False, \"unparseable\"\n","\n","        if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n","            return False, \"invalid_values\"\n","\n","    return True, \"valid\"\"\""]},{"cell_type":"code","execution_count":null,"id":"859c19b9","metadata":{"id":"859c19b9"},"outputs":[],"source":["def find_orphans(images_dir, labels_dir):\n","    \"\"\"Return (orphan_images, orphan_labels) given image/label directories.\"\"\"\n","    images_dir = Path(images_dir)\n","    labels_dir = Path(labels_dir)\n","\n","    image_exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n","    image_files = []\n","    for ext in image_exts:\n","        image_files.extend(images_dir.glob(f\"*{ext}\"))\n","\n","    label_files = list(labels_dir.glob(\"*.txt\"))\n","\n","    image_stems = {p.stem: p for p in image_files}\n","    label_stems = {p.stem: p for p in label_files}\n","\n","    orphan_image_stems = image_stems.keys() - label_stems.keys()\n","    orphan_label_stems = label_stems.keys() - image_stems.keys()\n","\n","    orphan_images = [image_stems[s] for s in orphan_image_stems]\n","    orphan_labels = [label_stems[s] for s in orphan_label_stems]\n","\n","    return orphan_images, orphan_labels"]},{"cell_type":"code","execution_count":null,"id":"f4e84b25","metadata":{"id":"f4e84b25"},"outputs":[],"source":["def clean_dataset(dataset_path, config):\n","    \"\"\"Validate labels, find orphans, and remove selected files. Returns stats dict.\"\"\"\n","    dataset_path = Path(dataset_path)\n","    images_dir = dataset_path / \"images\"\n","    labels_dir = dataset_path / \"labels\"\n","\n","    stats = {\n","        \"labels_removed\": 0,\n","        \"images_removed\": 0,\n","        \"empty_labels\": 0,\n","        \"wrong_format\": 0,\n","        \"unparseable\": 0,\n","        \"invalid_values\": 0,\n","        \"orphan_images\": 0,\n","        \"orphan_labels\": 0,\n","        \"files_to_remove\": [],\n","    }\n","\n","    if not dataset_path.exists():\n","        raise FileNotFoundError(f\"Dataset path does not exist: {dataset_path}\")\n","\n","    # Step 1: validate label files\n","    labels_to_remove = []\n","    if labels_dir.exists():\n","        label_files = list(labels_dir.glob(\"*.txt\"))\n","        print(f\"\\nChecking {len(label_files)} label files...\")\n","\n","        for label_path in tqdm(label_files, desc=\"labels\", unit=\"file\"):\n","            is_valid, reason = validate_label_file(label_path)\n","\n","            if not is_valid:\n","                should_remove = False\n","\n","                if reason == \"empty_file\" and config[\"remove_options\"][\"remove_empty_labels\"]:\n","                    stats[\"empty_labels\"] += 1\n","                    should_remove = True\n","                elif reason == \"wrong_format\" and config[\"remove_options\"][\"remove_wrong_format\"]:\n","                    stats[\"wrong_format\"] += 1\n","                    should_remove = True\n","                elif reason in (\"unparseable\", \"invalid_values\") and config[\"remove_options\"][\"remove_invalid_format\"]:\n","                    stats[reason] += 1\n","                    should_remove = True\n","\n","                if should_remove:\n","                    labels_to_remove.append(label_path)\n","                    stats[\"files_to_remove\"].append((\"label\", label_path, reason))\n","\n","        print(f\"  invalid labels scheduled for removal: {len(labels_to_remove)}\")\n","\n","    # Step 2: find orphaned files\n","    orphan_images, orphan_labels = find_orphans(images_dir, labels_dir)\n","    print(f\"\\nOrphans: {len(orphan_images)} images, {len(orphan_labels)} labels\")\n","\n","    if config[\"remove_options\"][\"remove_orphan_images\"]:\n","        stats[\"orphan_images\"] = len(orphan_images)\n","        for img in orphan_images:\n","            stats[\"files_to_remove\"].append((\"image\", img, \"orphan\"))\n","\n","    if config[\"remove_options\"][\"remove_orphan_labels\"]:\n","        stats[\"orphan_labels\"] = len(orphan_labels)\n","        for lbl in orphan_labels:\n","            stats[\"files_to_remove\"].append((\"label\", lbl, \"orphan\"))\n","            if lbl not in labels_to_remove:\n","                labels_to_remove.append(lbl)\n","\n","    # Step 3: apply removals\n","    print(\"\\nApplying removals...\")\n","\n","    # Remove labels and optionally corresponding images\n","    if labels_to_remove:\n","        image_exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n","        for label_path in tqdm(labels_to_remove, desc=\"labels\", unit=\"file\"):\n","            try:\n","                label_path.unlink()\n","                stats[\"labels_removed\"] += 1\n","\n","                if config[\"remove_options\"][\"remove_corresponding_files\"]:\n","                    for ext in image_exts:\n","                        image_path = images_dir / f\"{label_path.stem}{ext}\"\n","                        if image_path.exists():\n","                            image_path.unlink()\n","                            stats[\"images_removed\"] += 1\n","                            break\n","            except Exception as exc:\n","                print(f\"[warn] error removing {label_path}: {exc}\")\n","\n","    # Remove orphan images (if not already removed)\n","    if config[\"remove_options\"][\"remove_orphan_images\"]:\n","        for image_path in tqdm(orphan_images, desc=\"orphan images\", unit=\"file\"):\n","            try:\n","                if image_path.exists():\n","                    image_path.unlink()\n","                    stats[\"images_removed\"] += 1\n","            except Exception as exc:\n","                print(f\"[warn] error removing {image_path}: {exc}\")\n","\n","    return stats"]},{"cell_type":"code","execution_count":null,"id":"4979feb1","metadata":{"id":"4979feb1"},"outputs":[],"source":["def main():\n","    \"\"\"Run optional backup and dataset cleaning; return a results dict.\"\"\"\n","    results = {}\n","    dataset_dir = Path(CONFIG[\"dataset_path\"])\n","\n","    try:\n","        if CONFIG.get(\"create_backup\", False):\n","            backup_path = create_backup(dataset_dir, CONFIG[\"backup_dir\"])\n","            results[\"backup_path\"] = str(backup_path)\n","\n","        stats = clean_dataset(dataset_dir, CONFIG)\n","        results[\"clean_stats\"] = stats\n","\n","        print(\"\\nCleaning summary\")\n","        print(\"----------------\")\n","        print(f\"  labels removed : {stats['labels_removed']:,}\")\n","        print(f\"  images removed : {stats['images_removed']:,}\")\n","        if stats[\"empty_labels\"]:\n","            print(f\"  empty labels   : {stats['empty_labels']:,}\")\n","        if stats[\"wrong_format\"]:\n","            print(f\"  wrong format   : {stats['wrong_format']:,}\")\n","        if stats[\"unparseable\"]:\n","            print(f\"  unparseable    : {stats['unparseable']:,}\")\n","        if stats[\"invalid_values\"]:\n","            print(f\"  invalid values : {stats['invalid_values']:,}\")\n","        if stats[\"orphan_images\"]:\n","            print(f\"  orphan images  : {stats['orphan_images']:,}\")\n","        if stats[\"orphan_labels\"]:\n","            print(f\"  orphan labels  : {stats['orphan_labels']:,}\")\n","\n","        print(f\"\\nDataset location : {dataset_dir}\")\n","        if \"backup_path\" in results:\n","            print(f\"Backup location  : {results['backup_path']}\")\n","\n","    except Exception as exc:\n","        print(f\"\\nDataset cleaning failed: {exc}\")\n","        results[\"error\"] = str(exc)\n","\n","    return results\n","\n","\n","if __name__ == \"__main__\":\n","    cleaning_results = main()"]},{"cell_type":"code","execution_count":null,"id":"5aec62c6","metadata":{"id":"5aec62c6"},"outputs":[],"source":["dataset_path = Path(CONFIG[\"dataset_path\"])\n","images_dir = dataset_path / \"images\"\n","labels_dir = dataset_path / \"labels\"\n","\n","print(\"\\nFinal dataset state\")\n","print(\"-------------------\")\n","\n","if dataset_path.exists():\n","    image_files = list(images_dir.glob(\"*\")) if images_dir.exists() else []\n","    label_files = list(labels_dir.glob(\"*.txt\")) if labels_dir.exists() else []\n","\n","    n_img, n_lbl = len(image_files), len(label_files)\n","    print(f\"  images : {n_img:,}\")\n","    print(f\"  labels : {n_lbl:,}\")\n","    if n_img:\n","        cov = 100 * n_lbl / n_img if n_img else 0\n","        print(f\"  label/image: {cov:.1f}%\")\n","\n","    if image_files:\n","        print(\"\\n  sample images:\")\n","        for p in image_files[:5]:\n","            print(\"   -\", p.name)\n","        if n_img > 5:\n","            print(f\"   ... (+{n_img - 5:,} more)\")\n","else:\n","    print(f\"  dataset not found at {dataset_path}\")\n","\n","# Show impact from the last run, if available\n","if 'cleaning_results' in globals() and isinstance(cleaning_results, dict):\n","    stats = cleaning_results.get(\"clean_stats\", {})\n","    if stats:\n","        print(\"\\nCleaning impact\")\n","        print(\"---------------\")\n","        total_removed = stats.get(\"labels_removed\", 0) + stats.get(\"images_removed\", 0)\n","        print(f\"  labels removed : {stats.get('labels_removed', 0):,}\")\n","        print(f\"  images removed : {stats.get('images_removed', 0):,}\")\n","        print(f\"  total removed  : {total_removed:,}\")"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}